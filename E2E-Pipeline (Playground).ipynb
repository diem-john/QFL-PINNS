{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ba3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0932e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Configuration and Constants ---\n",
    "SEQUENCE_SIZE = 48  # Sequence length for PINN input\n",
    "TARGET_WINDOW = 24  # Forecast horizon\n",
    "TYPHOON_STEPS_IN = 12 # Sequence length for Seq2Seq input\n",
    "PROXIMITY_THRESHOLD_KM = 600 # Filter for historical data\n",
    "\n",
    "STATIONS = ['Guanyin', 'Keelung', 'Longtan', 'Taipei', 'Tamsui', 'Taoyuan', 'Yangmingshan']\n",
    "station_name = STATIONS[4]\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Feature Definitions\n",
    "WEATHER_FEATURES = ['air_pressure', 'temperature', 'relative_humidity', 'wind_speed', \n",
    "                    'wind_direction', 'gust_max', 'gust_max_dir', 'precipitation', 'solar_rad'] # 9 features\n",
    "TARGET_FEATURE = ['wind_speed'] \n",
    "TYPHOON_FEATURES_CORE = ['lat', 'lng', 'wind', 'long50'] # 4 core features for Seq2Seq\n",
    "TYPHOON_FEATURES_EXO_PRED = ['typhoon_lat', 'typhoon_lng', 'wind', 'long50', 'distance_km'] # 5 exogenous features for PINN\n",
    "\n",
    "# Full feature set for PINN input (9 weather + 5 exo = 14 features)\n",
    "PINN_ALL_FEATURES = WEATHER_FEATURES + TYPHOON_FEATURES_EXO_PRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Model Definitions ---\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    # Model structure provided by user\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, num_heads=1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, horizon_length):\n",
    "        encoder_output, (hidden, cell) = self.encoder(x)\n",
    "        decoder_input = torch.zeros(x.size(0), horizon_length, self.hidden_size, device=x.device)\n",
    "        decoder_out, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "        attention_output, _ = self.multihead_attention(query=decoder_out, key=encoder_output, value=encoder_output)\n",
    "        final_output = self.fc(attention_output)\n",
    "        return final_output\n",
    "\n",
    "class HybridModelPINN(nn.Module):\n",
    "    # Model structure provided by user\n",
    "    def __init__(self, input_size, output_size, sequence_length,\n",
    "                 embedding_dim=48, n_head=4, num_transformer_layers=4,\n",
    "                 conv_channels=64, kernel_size=3, pool_kernel=2,\n",
    "                 hidden_dense=512):\n",
    "        super(HybridModelPINN, self).__init__()\n",
    "        self.input_size = input_size \n",
    "        self.output_size = output_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.linear_projection = nn.Linear(input_size, embedding_dim)\n",
    "        self.positional_encoding = self._get_positional_encoding(sequence_length, embedding_dim)\n",
    "        self.register_buffer('positional_encoding_buffer', self.positional_encoding)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_head, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_transformer_layers)\n",
    "        self.conv_layer = nn.Conv1d(embedding_dim, conv_channels, kernel_size, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pooling_layer = nn.MaxPool1d(pool_kernel)\n",
    "\n",
    "        self.conv_output_length = (sequence_length + 2 * (kernel_size // 2) - (kernel_size - 1) - 1) + 1\n",
    "        self.pooled_output_length = self.conv_output_length // pool_kernel\n",
    "\n",
    "        self.dense1 = nn.Linear(conv_channels * self.pooled_output_length + embedding_dim * sequence_length, hidden_dense) \n",
    "        self.dense2 = nn.Linear(hidden_dense, output_size)\n",
    "        self.u_dense = nn.Linear(hidden_dense, output_size)\n",
    "        self.v_dense = nn.Linear(hidden_dense, output_size)\n",
    "        self.p_dense = nn.Linear(hidden_dense, output_size)\n",
    "\n",
    "    def _get_positional_encoding(self, seq_len, d_model):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(seq_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.squeeze(1).unsqueeze(0).transpose(1, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.linear_projection(x)\n",
    "        positional_encoding = self.positional_encoding_buffer[:, :embedded.size(1), :].transpose(1, 2) \n",
    "        embedded = embedded + positional_encoding.squeeze(2)\n",
    "\n",
    "        transformer_out = self.transformer_encoder(embedded)\n",
    "        transformer_out_flattened = transformer_out.view(transformer_out.size(0), -1)\n",
    "\n",
    "        cnn_in = embedded.permute(0, 2, 1)\n",
    "        conv_out = self.relu(self.conv_layer(cnn_in))\n",
    "        pooled_out = self.pooling_layer(conv_out)\n",
    "        pooled_out_flattened = pooled_out.view(pooled_out.size(0), -1)\n",
    "\n",
    "        combined_features = torch.cat((pooled_out_flattened, transformer_out_flattened), dim=1)\n",
    "        dense1_out = self.relu(self.dense1(combined_features))\n",
    "\n",
    "        output = self.dense2(dense1_out)\n",
    "        u_out = self.u_dense(dense1_out)\n",
    "        v_out = self.v_dense(dense1_out)\n",
    "        p_out = self.p_dense(dense1_out)\n",
    "\n",
    "        return output, u_out, v_out, p_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fc1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Utility Functions (Includes User's PINN Loss & Sequence Creation) ---\n",
    "\n",
    "def calculate_distance(row):    \n",
    "    try:\n",
    "        typhoon_loc = (row['lat'], row['lng'])\n",
    "    except:\n",
    "        typhoon_loc = (row['typhoon_lat'], row['typhoon_lng'])\n",
    "    \n",
    "    try:\n",
    "        station_loc = (row['station_latitude'], row['station_longitude'])\n",
    "    except:\n",
    "        station_loc = (row['latitude'], row['longitude'])\n",
    "        \n",
    "    if pd.isnull(typhoon_loc[0]) or pd.isnull(station_loc[0]):\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return haversine(typhoon_loc, station_loc, unit=Unit.KILOMETERS)\n",
    "    except ValueError as e:\n",
    "        corrected_longitude = typhoon_loc[1]\n",
    "        if corrected_longitude > 180: corrected_longitude = 180\n",
    "        elif corrected_longitude < -180: corrected_longitude = -180\n",
    "        corrected_typhoon_loc = (typhoon_loc[0], corrected_longitude)\n",
    "        return haversine(corrected_typhoon_loc, station_loc, unit=Unit.KILOMETERS)\n",
    "\n",
    "def create_sequences_typhoon(data_values, n_steps_in, n_steps_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_values)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > len(data_values):\n",
    "            break\n",
    "        seq_x = data_values[i:end_ix]\n",
    "        seq_y = data_values[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_sequence_weather(sequence_length, target_window, scaled_data_df):\n",
    "    target_column_name = 'wind_speed'\n",
    "    target_column_index = scaled_data_df.columns.get_loc(target_column_name)\n",
    "    feature_indices = [scaled_data_df.columns.get_loc(col) for col in scaled_data_df.columns if col != 'time']\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for i in tqdm(range(len(scaled_data_df) - sequence_length - target_window + 1), desc=\"Creating Sequences\"):\n",
    "        sequence = scaled_data_df.iloc[i:i+sequence_length, feature_indices].values\n",
    "        target = scaled_data_df.iloc[i+sequence_length:i+sequence_length+target_window, target_column_index].values\n",
    "        x.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "def physics_loss_fn(u, v, p, x):\n",
    "    # Stub function for physics loss in this environment\n",
    "    return torch.tensor(0.0) \n",
    "\n",
    "def run_seq2seq_training_and_forecast(typhoon_df, model_config, scaler_typhoon):\n",
    "    print(\"   -> Starting Seq2Seq Training and Forecasting...\")\n",
    "    # 1. Prepare sequences for Seq2Seq training\n",
    "    X_typhoon, y_typhoon = create_sequences_typhoon(typhoon_df[TYPHOON_FEATURES_CORE].values, TYPHOON_STEPS_IN, TARGET_WINDOW)\n",
    "    \n",
    "    # First split: 70% train, 30% temp (which will be split into val + test)\n",
    "    split_idx = int(0.7 * len(X_typhoon))\n",
    "    X_train, X_temp = X_typhoon[:split_idx], X_typhoon[split_idx:]\n",
    "    y_train, y_temp = y_typhoon[:split_idx], y_typhoon[split_idx:]\n",
    "\n",
    "    # Second split: split the temp set into 50% validation and 50% test\n",
    "    val_split_idx = int(0.5 * len(X_temp))\n",
    "\n",
    "    X_valid, X_test = X_temp[:val_split_idx], X_temp[val_split_idx:]\n",
    "    y_valid, y_test = y_temp[:val_split_idx], y_temp[val_split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float()\n",
    "    \n",
    "    X_valid_tensor = torch.from_numpy(X_valid).float()\n",
    "    y_valid_tensor = torch.from_numpy(y_valid).float()\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Step 3: Train the model (Simulated: 1 batch)\n",
    "    typhoon_model = Seq2Seq(**model_config)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(typhoon_model.parameters(), lr=0.001)\n",
    "    \n",
    "    \n",
    "    epochs = 1\n",
    "    early_stop_count = 0  \n",
    "    min_val_loss = float('inf')  # Initialize with a high value\n",
    "    val_loss_history = []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Typhoon Model Training & Validation\"):\n",
    "        typhoon_model.train()\n",
    "        for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "            optimizer.zero_grad()\n",
    "            output = typhoon_model(batch_x, TARGET_WINDOW)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        typhoon_model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\"):\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = typhoon_model(x_batch, TARGET_WINDOW)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_loss_history.append(val_loss)\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            print(f'[LOG] Early Stopping Counter: {early_stop_count}')\n",
    "\n",
    "        if early_stop_count >= 10:\n",
    "            print(f\"[LOG] Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "                           \n",
    "    # Step 4: Save the model\n",
    "    torch.save(typhoon_model.state_dict(), 'models/typhoon/typhoon_model.pth')\n",
    "    print(\"   -> Saved 'typhoon_model.pth'\")\n",
    "    \n",
    "    # Step 5: Generate forecast for all test sequences\n",
    "    typhoon_model.eval()\n",
    "    all_predictions = []\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor), batch_size=64, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x_test in test_loader:\n",
    "            output = typhoon_model(batch_x_test[0], TARGET_WINDOW).cpu().numpy()\n",
    "            all_predictions.append(output)\n",
    "            \n",
    "    raw_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "    # Inverse transform the predictions \n",
    "    predicted_physical_output = scaler_typhoon.inverse_transform(raw_predictions.reshape(-1, len(TYPHOON_FEATURES_CORE))).reshape(raw_predictions.shape)\n",
    "    \n",
    "    # Step 6: Create the full exogenous data (Actual input dates + Forecasted dates)\n",
    "    full_exogenous_df_list = []\n",
    "    \n",
    "    # The time indices for the input sequences of the test set\n",
    "    test_indices = np.arange(split_idx, len(typhoon_df) - TYPHOON_STEPS_IN - TARGET_WINDOW + 1)\n",
    "    \n",
    "    for i in range(raw_predictions.shape[0]):\n",
    "        # Get the time index corresponding to the start of the current test sequence\n",
    "        current_sequence_start_index = split_idx + i \n",
    "        \n",
    "        # This part of the data is the historical data for the input sequence itself\n",
    "        df_input = typhoon_df.iloc[current_sequence_start_index : current_sequence_start_index + TYPHOON_STEPS_IN].reset_index(drop=True)\n",
    "        \n",
    "        # This part is the predicted future data\n",
    "        input_end_time = typhoon_df['time'].iloc[current_sequence_start_index + TYPHOON_STEPS_IN - 1]\n",
    "        forecast_times = pd.date_range(start=input_end_time + pd.Timedelta(hours=1), periods=TARGET_WINDOW, freq='H')\n",
    "        \n",
    "        df_pred = pd.DataFrame(predicted_physical_output[i], columns=TYPHOON_FEATURES_CORE)\n",
    "        df_pred['time'] = forecast_times\n",
    "        df_pred['sequence_id'] = i  # Identifier for the sequence the forecast belongs to\n",
    "        \n",
    "        # Combine input sequence (historical) + forecast sequence (predicted)\n",
    "        df_full_seq = pd.concat([df_input, df_pred], ignore_index=True)\n",
    "        df_full_seq['sequence_id'] = i\n",
    "        full_exogenous_df_list.append(df_full_seq)\n",
    "        \n",
    "    return pd.concat(full_exogenous_df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true_scaled, y_pred_scaled, scaler_target):\n",
    "    # Flatten and Inverse Transform for R2 calculation on physical units\n",
    "    \n",
    "    # y_true_scaled is multi-step output, needs to be flattened: (N, 24) -> (N*24, 1)\n",
    "    y_true_flat_scaled = y_true_scaled.flatten().reshape(-1, 1)\n",
    "    y_pred_flat_scaled = y_pred_scaled.flatten().reshape(-1, 1)\n",
    "    \n",
    "    # Inverse transform to get true physical units\n",
    "    y_true = scaler_target.inverse_transform(y_true_flat_scaled)\n",
    "    y_pred = scaler_target.inverse_transform(y_pred_flat_scaled)\n",
    "\n",
    "    # Calculate Metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"R2\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae\n",
    "    }\n",
    "\n",
    "def run_pinn_training(weather_df, typhoon_data_hist_raw, station_coords, model_config):\n",
    "    print(f\"   -> Starting PINN Training for {station_name}...\")\n",
    "    \n",
    "    # 1. Load weather station data (done before call)\n",
    "    station_coords.rename(columns={'latitude': 'station_latitude', \n",
    "                                   'longitude': 'station_longitude', \n",
    "                                   'location': 'station'}, inplace=True)\n",
    "    \n",
    "    weather_df = pd.merge(weather_df, station_coords[['station', 'station_latitude', 'station_longitude']], on='station', how='left')\n",
    "    \n",
    "    # 2. Filter exogenous data: Combine historical weather with historical typhoon data\n",
    "    weather_exo_merge = pd.merge_asof(\n",
    "        weather_df.sort_values('time'),\n",
    "        typhoon_data_hist_raw[['time'] + TYPHOON_FEATURES_CORE].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    weather_exo_merge.rename(columns={'lat': 'typhoon_lat', 'lng': 'typhoon_lng'}, inplace=True)\n",
    "    weather_exo_merge['distance_km'] = weather_exo_merge.apply(calculate_distance, axis=1)\n",
    "    \n",
    "    # Apply 600km proximity filter: \n",
    "    weather_exo_merge['is_affected'] = (weather_exo_merge['distance_km'] <= PROXIMITY_THRESHOLD_KM).astype(int)\n",
    "    \n",
    "    # Create the final exogenous feature columns (5 features)\n",
    "    weather_exo_merge[TYPHOON_FEATURES_EXO_PRED] = weather_exo_merge[['typhoon_lat', 'typhoon_lng', 'wind', 'long50', 'distance_km']].copy()\n",
    "    \n",
    "    # 3. Combine weather station data and filtered exogenous data. Fill non-exogenous periods with 0.\n",
    "    final_pinn_df = weather_exo_merge[WEATHER_FEATURES + TYPHOON_FEATURES_EXO_PRED + ['time']].copy()\n",
    "    \n",
    "    # Set exogenous features to 0 for periods NOT affected by the typhoon (< 600km)\n",
    "    final_pinn_df.loc[weather_exo_merge['is_affected'] == 0, TYPHOON_FEATURES_EXO_PRED] = 0\n",
    "    \n",
    "    # Scaling (for PINN)\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "    \n",
    "    # Scale all 14 features\n",
    "    final_pinn_df[PINN_ALL_FEATURES] = scaler_features.fit_transform(final_pinn_df[PINN_ALL_FEATURES])\n",
    "    final_pinn_df[TARGET_FEATURE] = scaler_target.fit_transform(final_pinn_df[TARGET_FEATURE])\n",
    "    \n",
    "    # Prepare Sequences and DataLoaders for PINN training\n",
    "    X_pinn, y_pinn = create_sequence_weather(SEQUENCE_SIZE, TARGET_WINDOW, final_pinn_df)\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_pinn, y_pinn, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Step 4: Train the model (Simulated: 1 batch)\n",
    "    model_pinn = HybridModelPINN(**model_config)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_pinn.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, \n",
    "                              patience=3)\n",
    "    \n",
    "    # Training Loop\n",
    "    epochs = 1\n",
    "    early_stop_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    physics_loss_history = []  # Store physics loss\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Physics-Regulated Model Training and Validation\"):\n",
    "        model_pinn.train()\n",
    "        train_loss = []\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, u_pred, v_pred, p_pred = model_pinn(x_batch) #get the u,v,p predictions\n",
    "\n",
    "            # Data loss\n",
    "            data_loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Physics loss\n",
    "            physics_loss = physics_loss_fn(u_pred, v_pred, p_pred, x_batch)  # Pass x_batch\n",
    "\n",
    "            # Combine the losses with weights 0.73 and 0.27\n",
    "            loss = 0.73 * data_loss + 0.27 * physics_loss\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            physics_loss_history.append(physics_loss.item()) #store physics loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_history.append(np.mean(train_loss))\n",
    "\n",
    "        # Validation\n",
    "        model_pinn.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\"):\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs, _, _, _ = model_pinn(x_batch)  # No u,v,p for validation\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_loss_history.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            print(f'[LOG] Early Stopping Counter: {early_stop_count}')\n",
    "\n",
    "        if early_stop_count >= 10:\n",
    "            print(f\"[LOG] Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    model_pinn.eval()\n",
    "    X_test_tensor = X_test.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor, _, _, _ = model_pinn(X_test_tensor)\n",
    "        y_pred_scaled = y_pred_tensor.cpu().numpy()\n",
    "    metrics = calculate_metrics(y_test.numpy(), y_pred_scaled, scaler_target)\n",
    "    \n",
    "    # Step 5: Save the model\n",
    "    torch.save(model_pinn.state_dict(), f'models/weather_sta/{station_name.lower()}_pinn_model.pth')\n",
    "    print(f\"   -> Saved '{station_name.lower()}_pinn_model.pth'\")\n",
    "\n",
    "    return weather_exo_merge['is_affected'].sum(), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Typhoon Data\n",
    "typhoon_data_hist_raw = pd.read_csv('data/typhoon_data.csv', parse_dates=['Date'], infer_datetime_format=True)\n",
    "typhoon_data_hist_raw.rename(columns={'Date': 'time'}, inplace=True)\n",
    "\n",
    "# 2. Filter Typhoon Data to >= 2015\n",
    "typhoon_data_hist_filtered = typhoon_data_hist_raw[(typhoon_data_hist_raw['time'].dt.year >= 2015)].copy()\n",
    "typhoon_data_hist_filtered = typhoon_data_hist_filtered[TYPHOON_FEATURES_CORE + ['time']].copy()\n",
    "typhoon_data_hist_filtered.sort_values(by='time', inplace=True)\n",
    "typhoon_data_hist_filtered.dropna(inplace=True)\n",
    "\n",
    "# Scale the filtered typhoon data for training\n",
    "scaler_typhoon = MinMaxScaler(feature_range=(0, 1))\n",
    "typhoon_data_scaled_for_training = typhoon_data_hist_filtered.copy()\n",
    "typhoon_data_scaled_for_training[TYPHOON_FEATURES_CORE] = scaler_typhoon.fit_transform(typhoon_data_hist_filtered[TYPHOON_FEATURES_CORE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9de720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 颱風模式訓練 (Typhoon Model Training: Seq2Seq) ---\n",
      "   -> Starting Seq2Seq Training and Forecasting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e6259442604a578c3f09a973cf0108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Typhoon Model Training & Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b85e9b8bf2442ce9fe4626d7fdd0c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 (Train):   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05d922af1be469a8352d8e22ec3278b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 (Validation):   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Validation Loss: 0.0190\n",
      "   -> Saved 'typhoon_model.pth'\n",
      "\n",
      "✅ Typhoon Model Training & Forecasting Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 颱風模式訓練 (Typhoon Model Training: Seq2Seq) ---\")\n",
    "# 3, 4, 5, 6: Train, Save, Forecast, Generate Exogenous Data\n",
    "seq2seq_config = {\n",
    "    \"input_size\": len(TYPHOON_FEATURES_CORE), \n",
    "    \"hidden_size\": 128, \n",
    "    \"num_layers\": 2, \n",
    "    \"output_size\": len(TYPHOON_FEATURES_CORE), \n",
    "    \"num_heads\": 4\n",
    "}\n",
    "\n",
    "full_forecast_exogenous_df = run_seq2seq_training_and_forecast(\n",
    "    typhoon_data_scaled_for_training, seq2seq_config, scaler_typhoon\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Typhoon Model Training & Forecasting Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f3e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 個別氣象站模型訓練 (Individual Station Training: Hybrid PINN) ---\n",
      "   -> Starting PINN Training for Tamsui...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5eb55dffa464c7e9b85d6961792e11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Sequences:   0%|          | 0/9929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9992c377e53c42d483f3f7b24bfbd6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Physics-Regulated Model Training and Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a314746fa4c84116bb396d1f631af123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 (Train):   0%|          | 0/497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01e4ff7bb9c48d59f1947329c1d790a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 (Validation):   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Validation Loss: 0.0011\n",
      "   -> Saved 'tamsui_pinn_model.pth'\n",
      "\n",
      "✅ Individual Station Model Training Complete for Tamsui.\n",
      "Total historical typhoon instances within 600km threshold: 189 records.\n",
      "\n",
      "--- Hybrid PINN Model Metric Scores (Test Set) ---\n",
      "R2 Score (R2): -7.107145\n",
      "Root Mean Squared Error (RMSE): 0.031318\n",
      "Mean Squared Error (MSE): 0.000981\n",
      "Mean Absolute Error (MAE): 0.027842\n",
      "\n",
      "--- Saved Models (Simulated) ---\n",
      "typhoon_model.pth (Typhoon Track Forecasting Model)\n",
      "tamsui_pinn_model.pth (Weather Station Wind Speed Model)\n"
     ]
    }
   ],
   "source": [
    "# --- Individual Weather Station Workflow ---\n",
    "print(\"\\n--- 個別氣象站模型訓練 (Individual Station Training: Hybrid PINN) ---\")\n",
    "\n",
    "# Load data\n",
    "station_coords = pd.read_csv('data/weather_station_coords.csv')\n",
    "station_coords.rename(columns={'lat': 'station_latitude', 'long': 'station_longitude', 'location': 'station'}, inplace=True)\n",
    "weather_sta_df = pd.read_csv(f'data/indiv_weather_station/{station_name}.csv').fillna(0)\n",
    "\n",
    "# Process data\n",
    "time_col = [col for col in weather_sta_df.columns if 'time' in col.lower()][0]\n",
    "weather_sta_df['time'] = pd.to_datetime(weather_sta_df[time_col], format='mixed', dayfirst=True)\n",
    "weather_sta_df.columns = [f'time_{station_name}', 'air_pressure', 'temperature',\n",
    "                      'relative_humidity', 'wind_speed',\n",
    "                      'wind_direction', 'gust_max', 'gust_max_dir',\n",
    "                      'precipitation', 'solar_rad', 'time']\n",
    "weather_sta_df.drop(columns=[f'time_{station_name}'], inplace=True)\n",
    "weather_sta_df = weather_sta_df[(weather_sta_df['time'].dt.year >= 2015)].copy()\n",
    "weather_sta_df['station'] = station_name\n",
    "\n",
    "pinn_config = {\n",
    "    \"input_size\": len(PINN_ALL_FEATURES), \n",
    "    \"output_size\": TARGET_WINDOW, \n",
    "    \"sequence_length\": SEQUENCE_SIZE, \n",
    "    \"hidden_dense\": 512\n",
    "}\n",
    "\n",
    "affected_count, pinn_metrics = run_pinn_training(weather_sta_df[-10000:], typhoon_data_hist_raw, station_coords, pinn_config)\n",
    "\n",
    "print(f\"\\n✅ Individual Station Model Training Complete for {station_name}.\")\n",
    "print(f\"Total historical typhoon instances within {PROXIMITY_THRESHOLD_KM}km threshold: {affected_count} records.\")\n",
    "\n",
    "# --- Metric Scores ---\n",
    "print(\"\\n--- Hybrid PINN Model Metric Scores (Test Set) ---\")\n",
    "print(f\"R2 Score (R2): {pinn_metrics['R2']:.6f}\")\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {pinn_metrics['RMSE']:.6f}\")\n",
    "print(f\"Mean Squared Error (MSE): {pinn_metrics['MSE']:.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {pinn_metrics['MAE']:.6f}\")\n",
    "\n",
    "print(\"\\n--- Saved Models (Simulated) ---\")\n",
    "print(\"typhoon_model.pth (Typhoon Track Forecasting Model)\")\n",
    "print(f\"{station_name.lower()}_pinn_model.pth (Weather Station Wind Speed Model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e201534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Individual Station Model Training Complete for Tamsui.\n",
      "Total historical typhoon instances within 600km threshold: 189 records.\n",
      "\n",
      "--- Hybrid PINN Model Metric Scores (Test Set) ---\n",
      "Root Mean Squared Error (RMSE): 0.031318\n",
      "Mean Squared Error (MSE): 0.000981\n",
      "Mean Absolute Error (MAE): 0.027842\n",
      "\n",
      "--- Saved Models (Simulated) ---\n",
      "typhoon_model.pth (Typhoon Track Forecasting Model)\n",
      "tamsui_pinn_model.pth (Weather Station Wind Speed Model)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ Individual Station Model Training Complete for {station_name}.\")\n",
    "print(f\"Total historical typhoon instances within {PROXIMITY_THRESHOLD_KM}km threshold: {affected_count} records.\")\n",
    "\n",
    "# --- Metric Scores ---\n",
    "print(\"\\n--- Hybrid PINN Model Metric Scores (Test Set) ---\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {pinn_metrics['RMSE']:.6f}\")\n",
    "print(f\"Mean Squared Error (MSE): {pinn_metrics['MSE']:.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {pinn_metrics['MAE']:.6f}\")\n",
    "\n",
    "print(\"\\n--- Saved Models (Simulated) ---\")\n",
    "print(\"typhoon_model.pth (Typhoon Track Forecasting Model)\")\n",
    "print(f\"{station_name.lower()}_pinn_model.pth (Weather Station Wind Speed Model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "869a42de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>grade</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind</th>\n",
       "      <th>dir50</th>\n",
       "      <th>long50</th>\n",
       "      <th>short50</th>\n",
       "      <th>dir30</th>\n",
       "      <th>long30</th>\n",
       "      <th>short30</th>\n",
       "      <th>intp</th>\n",
       "      <th>seq_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222447</th>\n",
       "      <td>2022-11-13 04:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>22.06</td>\n",
       "      <td>165.82</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222448</th>\n",
       "      <td>2022-11-13 05:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>22.08</td>\n",
       "      <td>165.81</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222449</th>\n",
       "      <td>2022-11-13 06:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>22.10</td>\n",
       "      <td>165.80</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>202224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222450</th>\n",
       "      <td>2022-11-13 07:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>22.11</td>\n",
       "      <td>165.79</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222451</th>\n",
       "      <td>2022-11-13 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>22.12</td>\n",
       "      <td>165.78</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222542</th>\n",
       "      <td>2022-12-12 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>20.40</td>\n",
       "      <td>131.30</td>\n",
       "      <td>1003.3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222543</th>\n",
       "      <td>2022-12-12 09:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>20.38</td>\n",
       "      <td>131.38</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222544</th>\n",
       "      <td>2022-12-12 10:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>20.36</td>\n",
       "      <td>131.44</td>\n",
       "      <td>1004.7</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222545</th>\n",
       "      <td>2022-12-12 11:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>20.33</td>\n",
       "      <td>131.47</td>\n",
       "      <td>1005.3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>202225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222546</th>\n",
       "      <td>2022-12-12 12:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>20.30</td>\n",
       "      <td>131.50</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time  grade    lat     lng  pressure  wind  dir50  \\\n",
       "222447 2022-11-13 04:00:00      3  22.06  165.82    1004.0  35.0      0   \n",
       "222448 2022-11-13 05:00:00      3  22.08  165.81    1004.0  35.0      0   \n",
       "222449 2022-11-13 06:00:00      3  22.10  165.80    1004.0  35.0      0   \n",
       "222450 2022-11-13 07:00:00      3  22.11  165.79    1004.0  35.0      0   \n",
       "222451 2022-11-13 08:00:00      3  22.12  165.78    1004.0  35.0      0   \n",
       "...                    ...    ...    ...     ...       ...   ...    ...   \n",
       "222542 2022-12-12 08:00:00      3  20.40  131.30    1003.3  35.0      0   \n",
       "222543 2022-12-12 09:00:00      3  20.38  131.38    1004.0  35.0      0   \n",
       "222544 2022-12-12 10:00:00      3  20.36  131.44    1004.7  35.0      0   \n",
       "222545 2022-12-12 11:00:00      3  20.33  131.47    1005.3  35.0      0   \n",
       "222546 2022-12-12 12:00:00      6  20.30  131.50    1006.0   0.0      0   \n",
       "\n",
       "        long50  short50  dir30  long30  short30  intp  seq_id  \n",
       "222447       0        0      1     180       90     1  202224  \n",
       "222448       0        0      1     180       90     1  202224  \n",
       "222449       0        0      1     180       90     0  202224  \n",
       "222450       0        0      1     180       90     1  202224  \n",
       "222451       0        0      1     180       90     1  202224  \n",
       "...        ...      ...    ...     ...      ...   ...     ...  \n",
       "222542       0        0      8     150       90     1  202225  \n",
       "222543       0        0      8     150       90     1  202225  \n",
       "222544       0        0      8     150       90     1  202225  \n",
       "222545       0        0      8     150       90     1  202225  \n",
       "222546       0        0      0       0        0     0  202225  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typhoon_data_hist_raw[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46780df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical typhoon instances within 600km threshold from Tamsui: 189 records.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Historical typhoon instances within {PROXIMITY_THRESHOLD_KM}km threshold from {station_name}: {affected_count} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77aab5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Individual Station Model Training Complete for Tamsui.\n",
      "\n",
      "--- Hybrid PINN Model Metric Scores (Test Set) ---\n",
      "R2 Score (R2): -7.107145\n",
      "Root Mean Squared Error (RMSE): 0.031318\n",
      "Mean Squared Error (MSE): 0.000981\n",
      "Mean Absolute Error (MAE): 0.027842\n",
      "\n",
      "--- Saved Models (Simulated) ---\n",
      "typhoon_model.pth (Typhoon Track Forecasting Model)\n",
      "tamsui_pinn_model.pth (Weather Station Wind Speed Model)\n",
      "Metrics Scores for Tamsui is saved!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ Individual Station Model Training Complete for {station_name}.\")\n",
    "\n",
    "try:\n",
    "    pinn_metrics['RMSE'] = pinn_metrics['RMSE'].item()\n",
    "except:\n",
    "    pass\n",
    "pd.DataFrame([pinn_metrics]).to_csv(f'models/weather_sta/metadata/{station_name}_metrics.csv')\n",
    "\n",
    "# --- Metric Scores ---\n",
    "print(\"\\n--- Hybrid PINN Model Metric Scores (Test Set) ---\")\n",
    "print(f\"R2 Score (R2): {pinn_metrics['R2']:.6f}\")\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {pinn_metrics['RMSE']:.6f}\")\n",
    "print(f\"Mean Squared Error (MSE): {pinn_metrics['MSE']:.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {pinn_metrics['MAE']:.6f}\")\n",
    "\n",
    "print(\"\\n--- Saved Models (Simulated) ---\")\n",
    "print(\"typhoon_model.pth (Typhoon Track Forecasting Model)\")\n",
    "print(f\"{station_name.lower()}_pinn_model.pth (Weather Station Wind Speed Model)\")\n",
    "print(f\"Metrics Scores for {station_name} is saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c45097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f3296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j3_env",
   "language": "python",
   "name": "j3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
